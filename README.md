# Языковая сложность в мультиязыковых моделях

Иллюстрационный материал к работе Языковая сложность в мультиязыковых моделях. Основной репозиторий с [экспериментом](https://github.com/AndreyK2001/BERT-Classification)

## Задача

Изучить поведение скрытого пространства BERT при переносе обучения между языками (для задачи предсказания языковой сложности)

## Цель

Найти метрику (метрики), корректно отражающие процессы в скрытых пространствах BERT в задаче языкового переноса

## Выводы
1. На задачах XNLI и языковой сложности на ReadMe++ метрика ANC убывает в ряду языков французский, русский, арабский, хинди

2. Метрика ANC и расстояние Вассерштайна дают представление о процессах, происходящих на слоях BERT

3. До 7 слоя BERT вектора «выравниваются», затем затем изменяется распределение координат без изменения направления векторов

4. Метрика ANC коррелирует с качеством переноса обучения модели

## Литература

1. Naous T. [et al.]. ReadMe++: Benchmarking Multilingual Language Models for Multi-Domain Readability Assessment // arXiv preprint arXiv:2305.14463. 2023.

2. Maksym D., Mark F., Cross-lingual Similarity of Multilingual Representations Revisited // Association for Computational Linguistics. 2022. V.  1. P.  185–195.
